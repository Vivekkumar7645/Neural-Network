{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Q 1. What is deep learning, and how is it connected to artificial intelligence.\n",
        "**Ans** - Deep learning is a subfield of Machine learning, which itself is a core part of Artificial intelligence.\n",
        "\n",
        "1. **Artificial Intelligence**:\n",
        "  * Broad field aiming to create machines that can mimic human intelligence.\n",
        "  * Includes problem-solving, reasoning, learning, perception, language understanding, etc.\n",
        "  * Example: Self-driving cars, chatbots, recommendation systems.\n",
        "\n",
        "2. **Machine Learning** - a subset of AI:\n",
        "  * Focuses on systems that can **learn from data** and improve over time without being explicitly programmed.\n",
        "  * Uses algorithms to find patterns in data and make decisions or predictions.\n",
        "\n",
        "3. **Deep Learning** - a subset of ML:\n",
        "  * Based on artificial neural networks inspired by the human brain.\n",
        "  * Involves multiple layers that enable the model to learn complex patterns.\n",
        "  * Especially powerful in tasks like image recognition, speech processing, and natural\n",
        "   language understanding.\n",
        "\n",
        "Relationship between them:"
      ],
      "metadata": {
        "id": "89POujL3dYXP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Artificial Intelligence\n",
        "└── Machine Learning\n",
        "    └── Deep Learning"
      ],
      "metadata": {
        "id": "oGE9NQqZGNup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Real-world examples of Deep Learning:**\n",
        "* Voice assistants like Siri or Alexa\n",
        "* Facial recognition systems\n",
        "* Language translation apps\n",
        "* Autonomous vehicles"
      ],
      "metadata": {
        "id": "m3A6d-zDGS7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 2. What is a neural network, and what are the different types of neural networks?\n",
        "**Ans** -A neural network is a computational model inspired by the structure and function of the human brain. It is made up of layers of nodes that work together to learn patterns from data.\n",
        "\n",
        "Each neuron receives inputs, processes them, and passes the result to the next layer. Neural networks \"learn\" by adjusting these weights during training to minimize error in predictions.\n",
        "\n",
        "**Basic Structure of a Neural Network:**\n",
        "1. Input Layer - Receives the raw data.\n",
        "2. Hidden Layers - Intermediate layers that process inputs through learned weights and activations.\n",
        "3. Output Layer - Produces the final prediction or classification.\n",
        "\n",
        "**Types of Neural Networks:**\n",
        "1. Feedforward Neural Network\n",
        "  * Information moves in one direction: input → hidden layers → output.\n",
        "  * Most basic type.\n",
        "  * Used in simple classification and regression tasks.\n",
        "\n",
        "2. Convolutional Neural Network\n",
        "  * Specialized for image and video data.\n",
        "  * Uses convolutional layers to detect spatial features (like edges, textures).\n",
        "  * Applications: image classification, object detection, face recognition.\n",
        "\n",
        "3. Recurrent Neural Network\n",
        "  * Designed for sequence data (e.g., time series, text).\n",
        "  * Has feedback loops that allow information to persist.\n",
        "  * Used in: language modeling, speech recognition, stock prediction.\n",
        "\n",
        "Long Short-Term Memory and Gated Recurrent Unit are advanced types of RNNs that solve issues like vanishing gradients.\n",
        "\n",
        "4. Generative Adversarial Network\n",
        "  * Two networks: a generator and a discriminator.\n",
        "  * Generator creates fake data, discriminator tries to distinguish it from real data.\n",
        "  * Used for: image generation, deepfakes, data augmentation.\n",
        "\n",
        "5. Radial Basis Function Neural Network\n",
        "  * Uses radial basis functions as activation.\n",
        "  * Good for function approximation and time-series prediction.\n",
        "\n",
        "6. Modular Neural Network\n",
        "  * Consists of several independent neural networks that work together.\n",
        "  * Useful when breaking complex tasks into smaller sub-problems.\n",
        "\n",
        "7. Transformer Networks\n",
        "  * Built using attention mechanisms instead of recurrence or convolution.\n",
        "  * Dominant in natural language processing (e.g., GPT, BERT).\n",
        "  * Great for translation, text summarization, question answering, etc.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "| Type | Best For | Key Feature |\n",
        "|-|||\n",
        "| Feedforward (FNN) | General tasks | One-way flow of data |\n",
        "| Convolutional (CNN) | Images, videos | Convolution layers extract features |\n",
        "| Recurrent (RNN, LSTM) | Sequences (text, time-series) | Loops to retain past information |\n",
        "| GAN | Data generation | Generator vs. Discriminator model |\n",
        "| RBFN | Function approximation | Radial activation functions |\n",
        "| MNN | Complex problems | Combines multiple networks |\n",
        "| Transformer | NLP, sequence modeling | Self-attention, parallel processing |"
      ],
      "metadata": {
        "id": "62YeJiTbdi0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 3. What is the mathematical structure of a neural network?\n",
        "**Ans** - The mathematical structure of a neural network is built on linear algebra, calculus, and optimization techniques. At its core, it involves matrix multiplications, activation functions, and weight updates through backpropagation.\n",
        "\n",
        "1. **Neural Network Building Block: The Neuron**\n",
        "\n",
        "A single artificial neuron performs the following operations\n",
        "\n",
        "**Equation**\n",
        "\n",
        "$$\n",
        "z = w_1x_1 + w_2x_2 + \\dots + w_nx_n + b = \\mathbf{w}^T \\mathbf{x} + b\n",
        "$$\n",
        "\n",
        "$$\n",
        "a = \\phi(z)\n",
        "$$\n",
        "\n",
        "* $\\mathbf{x} = [x_1, x_2, \\dots, x_n]$: Input vector\n",
        "* $\\mathbf{w} = [w_1, w_2, \\dots, w_n]$: Weight vector\n",
        "* $b$: Bias term\n",
        "* $z$: Weighted sum (pre-activation)\n",
        "* $\\phi(z)$: Activation function (like sigmoid, ReLU, tanh)\n",
        "* $a$: Output of the neuron (activation)\n",
        "\n",
        "2. **Layer-wise Structure (Vectorized Form)**\n",
        "\n",
        "For layer $l$ in a neural network:\n",
        "\n",
        "**Forward Propagation Equation:**\n",
        "\n",
        "$$\n",
        "\\mathbf{z}^{[l]} = \\mathbf{W}^{[l]} \\mathbf{a}^{[l-1]} + \\mathbf{b}^{[l]}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{a}^{[l]} = \\phi(\\mathbf{z}^{[l]})\n",
        "$$\n",
        "\n",
        "* $\\mathbf{W}^{[l]}$: Weight matrix for layer $l$\n",
        "* $\\mathbf{b}^{[l]}$: Bias vector for layer $l$\n",
        "* $\\mathbf{a}^{[l-1]}$: Activations from previous layer\n",
        "* $\\mathbf{a}^{[l]}$: Activations for current layer\n",
        "\n",
        "3. **Activation Functions (Non-linear Transformations)**\n",
        "\n",
        "| Name | Formula | Output Range |\n",
        "|-|||\n",
        "| Sigmoid | $\\sigma(z) = \\frac{1}{1 + e^{-z}}$                     | (0, 1)            |\n",
        "| Tanh    | $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$         | (–1, 1)           |\n",
        "| ReLU    | $\\text{ReLU}(z) = \\max(0, z)$                          | \\[0, ∞)           |\n",
        "| Softmax | $\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}$ | (0, 1), sums to 1 |\n",
        "\n",
        "4. **Loss Function**\n",
        "\n",
        "Used to measure error between prediction and true label.\n",
        "\n",
        "Examples:\n",
        "\n",
        "  * **Mean Squared Error (MSE)** (for regression):\n",
        "\n",
        "  $$\n",
        "  \\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n",
        "  $$\n",
        "\n",
        "  * **Cross-Entropy Loss** (for classification):\n",
        "\n",
        "  $$\n",
        "  \\text{Loss} = -\\sum y \\log(\\hat{y})\n",
        "  $$\n",
        "\n",
        "5. **Backpropagation and Gradient Descent**\n",
        "\n",
        "To train the network, we use gradient descent to minimize the loss.\n",
        "\n",
        "**Weight Update Rule**\n",
        "\n",
        "$$\n",
        "W^{[l]} \\leftarrow W^{[l]} - \\alpha \\frac{\\partial \\text{Loss}}{\\partial W^{[l]}}\n",
        "$$\n",
        "\n",
        "* $\\alpha$: Learning rate\n",
        "* $\\frac{\\partial \\text{Loss}}{\\partial W^{[l]}}$: Gradient of the loss w\\.r.t. weights\n",
        "\n",
        "Backpropagation uses the chain rule of calculus to compute gradients efficiently from the output layer back to the input.\n",
        "\n",
        "**Full Example: 1 Hidden Layer Neural Network**\n",
        "\n",
        "**Forward Pass:**\n",
        "\n",
        "$$\n",
        "\\text{Input: } \\mathbf{x}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{z}^{[1]} = \\mathbf{W}^{[1]} \\mathbf{x} + \\mathbf{b}^{[1]}, \\quad \\mathbf{a}^{[1]} = \\phi(\\mathbf{z}^{[1]})\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{z}^{[2]} = \\mathbf{W}^{[2]} \\mathbf{a}^{[1]} + \\mathbf{b}^{[2]}, \\quad \\hat{y} = \\phi(\\mathbf{z}^{[2]})\n",
        "$$\n",
        "\n",
        "**Summary**\n",
        "\n",
        "| Component | Role |\n",
        "|-||\n",
        "| Vectors & Matrices   | Represent inputs, weights, activations |\n",
        "| Activation Functions | Introduce non-linearity                |\n",
        "| Loss Function        | Measures error                         |\n",
        "| Backpropagation      | Computes gradients                     |\n",
        "| Gradient Descent     | Optimizes weights                      |"
      ],
      "metadata": {
        "id": "Opv3jyM6djuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 4. What is an activation function, and why is it essential in neural network?\n",
        "**Ans** - An activation function is a mathematical function applied to the output of each neuron in a neural network. It introduces non-linearity into the network, allowing it to learn complex patterns and relationships in data.\n",
        "\n",
        "It is Essential in a Neural Network, Without activation functions, a neural network would be just a stack of linear transformations, meaning:\n",
        "\n",
        "> No matter how many layers you add, the output would still be a linear function of the input. That severely limits the network's ability to model complex data.\n",
        "\n",
        "Activation functions enable the network to learn non-linear mappings - the key to tasks like image recognition, language translation, and more.\n",
        "\n",
        "**Functions of Activation**\n",
        "\n",
        "| Purpose | Explanation |\n",
        "|-||\n",
        "| Introduce Non-linearity | Helps the network approximate any complex function     |\n",
        "| Control Output Range                 | Keeps values bounded (e.g., between 0 and 1)           |\n",
        "| Allow Deep Learning                  | Enables deeper networks to learn hierarchical features |\n",
        "| Gradient Flow During Backpropagation | Some functions help maintain useful gradients          |\n",
        "\n",
        "**Common Activation Functions**\n",
        "\n",
        "| Name | Formula | Output Range | Use Case |\n",
        "|-||||\n",
        "| **Sigmoid**    | $\\sigma(z) = \\frac{1}{1 + e^{-z}}$             | (0, 1)            | Binary classification output              |\n",
        "| **Tanh**       | $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ | (–1, 1)           | Hidden layers                             |\n",
        "| **ReLU**       | $\\text{ReLU}(z) = \\max(0, z)$                  | \\[0, ∞)           | Most hidden layers (fast & efficient)     |\n",
        "| **Leaky ReLU** | $\\max(0.01z, z)$                               | (–∞, ∞)           | Avoids dying neurons in ReLU              |\n",
        "| **Softmax**    | $\\frac{e^{z_i}}{\\sum_j e^{z_j}}$               | (0, 1), sums to 1 | Final layer in multi-class classification |\n",
        "\n",
        "**Without Activation (Why It Fails)**\n",
        "\n",
        "$$\n",
        "\\text{Output} = W_3(W_2(W_1 \\cdot x)) = W \\cdot x \\quad \\text{(still linear)}\n",
        "$$\n",
        "\n",
        "With Activation:\n",
        "\n",
        "$$\n",
        "\\text{Output} = \\phi_3(W_3 \\cdot \\phi_2(W_2 \\cdot \\phi_1(W_1 \\cdot x)))\n",
        "$$\n",
        "\n",
        "It can model non-linear and complex relationships."
      ],
      "metadata": {
        "id": "JH6XIOMBdj_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 5. Could we list some common activation functions used in neural networks?\n",
        "**Ans** - List of common activation functions used in neural networks, along with their formulas, characteristics, and typical use cases:\n",
        "\n",
        "1. **Sigmoid**\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "* Range: (0, 1)\n",
        "* Pros: Smooth gradient; used for binary classification output\n",
        "* Cons: Vanishing gradient problem; not zero-centered\n",
        "* Use Case: Output layer in binary classification\n",
        "\n",
        "2. **Tanh**\n",
        "\n",
        "$$\n",
        "\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
        "$$\n",
        "\n",
        "* Range: (–1, 1)\n",
        "* Pros: Zero-centered output; stronger gradients than sigmoid\n",
        "* Cons: Still suffers from vanishing gradients\n",
        "* Use Case: Hidden layers (sometimes)\n",
        "\n",
        "3. **ReLU**\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(z) = \\max(0, z)\n",
        "$$\n",
        "\n",
        "* Range: \\[0, ∞)\n",
        "* Pros: Fast to compute; sparse activation; avoids vanishing gradient\n",
        "* Cons: \"Dying ReLU\" problem (neurons can become inactive)\n",
        "* Use Case: Most hidden layers in deep networks\n",
        "\n",
        "4. **Leaky ReLU**\n",
        "\n",
        "$$\n",
        "\\text{LeakyReLU}(z) = \\begin{cases}\n",
        "z & \\text{if } z \\geq 0 \\\\\n",
        "\\alpha z & \\text{if } z < 0\n",
        "\\end{cases}\n",
        "\\quad (\\alpha \\approx 0.01)\n",
        "$$\n",
        "\n",
        "* Range: (–∞, ∞)\n",
        "* Pros: Fixes dying ReLU issue by allowing small gradients for negative inputs\n",
        "* Use Case: Hidden layers in deep networks\n",
        "\n",
        "5. **Parametric ReLU**\n",
        "\n",
        "$$\n",
        "\\text{PReLU}(z) = \\begin{cases}\n",
        "z & \\text{if } z \\geq 0 \\\\\n",
        "a z & \\text{if } z < 0\n",
        "\\end{cases}\n",
        "\\quad \\text{where } a \\text{ is learned}\n",
        "$$\n",
        "\n",
        "* Range: (–∞, ∞)\n",
        "* Pros: Adaptable negative slope; more flexible than Leaky ReLU\n",
        "* Use Case: Deep learning models (e.g., CNNs)\n",
        "\n",
        "6. **ELU**\n",
        "\n",
        "$$\n",
        "\\text{ELU}(z) = \\begin{cases}\n",
        "z & \\text{if } z \\geq 0 \\\\\n",
        "\\alpha (e^z - 1) & \\text{if } z < 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "* Range: (-α, ∞)\n",
        "* Pros: Smooth curve; helps learning faster\n",
        "* Use Case: Advanced deep learning tasks\n",
        "\n",
        "7. **Softmax**\n",
        "\n",
        "$$\n",
        "\\text{Softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
        "$$\n",
        "\n",
        "* Range: (0, 1), outputs sum to 1\n",
        "* Pros: Converts raw scores to probabilities\n",
        "* Use Case: Final layer in multi-class classification\n",
        "\n",
        "8. **Swish**\n",
        "\n",
        "$$\n",
        "\\text{Swish}(z) = z \\cdot \\sigma(z)\n",
        "$$\n",
        "\n",
        "* Range: (-0.28, ∞)\n",
        "* Pros: Smooth, non-monotonic; outperforms ReLU in some deep networks\n",
        "* Use Case: High-performance deep learning models (e.g., image recognition)"
      ],
      "metadata": {
        "id": "zoBztbwldkQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 6. What is a multilayer neural network?\n",
        "**Ans** - A multilayer neural network is a type of artificial neural network that consists of more than one layer of neurons between the input and output layers. These layers enable the network to learn complex, non-linear relationships in the data.\n",
        "\n",
        "**Structure of a Multilayer Neural Network**\n",
        "\n",
        "```\n",
        "Input Layer → Hidden Layer → Output Layer\n",
        "```\n",
        "\n",
        "* Input Layer: Receives raw data.\n",
        "* Hidden Layers: Perform transformations on the input data using weights, biases, and activation functions.\n",
        "* Output Layer: Produces the final prediction.\n",
        "\n",
        "Each neuron in a layer is typically fully connected to all neurons in the next layer.\n",
        "\n",
        "**Mathematical Representation**\n",
        "\n",
        "For a single hidden layer:\n",
        "\n",
        "$$\n",
        "\\text{Hidden output (h)} = f(W_1 \\cdot x + b_1)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{Final output (y)} = g(W_2 \\cdot h + b_2)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $x$: Input vector\n",
        "* $W_1, W_2$: Weight matrices\n",
        "* $b_1, b_2$: Bias vectors\n",
        "* $f, g$: Activation functions (e.g., ReLU, Sigmoid)\n",
        "\n",
        "In deeper networks, this pattern repeats over multiple hidden layers.\n",
        "\n",
        "**Use of Multiple Layers**\n",
        "\n",
        "* Single-layer networks can only learn linearly separable functions.\n",
        "* Multilayer networks can learn non-linear and hierarchical features.\n",
        "* Deep networks with many layers form the foundation of deep learning.\n",
        "\n",
        "**Example Use Cases**\n",
        "\n",
        "| Application | Description |\n",
        "| -------------------- | ------------------------------------------- |\n",
        "| Image classification | Recognizing digits, objects, or faces       |\n",
        "| NLP tasks            | Text classification, sentiment analysis     |\n",
        "| Forecasting          | Time-series prediction (e.g., stock prices) |\n",
        "| Game AI              | Decision-making in complex environments     |"
      ],
      "metadata": {
        "id": "tpxnDhPgdkhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 7. What is a loss function, and why is it crucial for neural network training?\n",
        "**Ans** - A loss function is a mathematical function that measures how well a neural network's predictions match the actual target values during training.\n",
        "\n",
        "It calculates the difference or error between the predicted output by the network and the true output.\n",
        "\n",
        "It is Crucial for Neural Network Training\n",
        "\n",
        "1. **Guides Learning:**\n",
        "\n",
        "   * The loss function quantifies the network's prediction error.\n",
        "   * During training, the goal is to minimize this loss by adjusting the model's parameters.\n",
        "\n",
        "2. **Feedback for Optimization:**\n",
        "\n",
        "   * Loss values are used by optimization algorithms to compute gradients.\n",
        "   * These gradients tell the network how to update weights to reduce the error.\n",
        "\n",
        "3. **Training Progress Indicator:**\n",
        "\n",
        "   * Monitoring the loss function during training shows how well the network is learning.\n",
        "   * A decreasing loss indicates improving performance.\n",
        "\n",
        "Example:\n",
        "\n",
        "* If the loss is high, predictions are poor.\n",
        "* If the loss is low, predictions are close to actual values.\n",
        "\n",
        "**Common Loss Functions**\n",
        "\n",
        "| Problem Type | Loss Function | Description |\n",
        "|-|||\n",
        "| Regression | Mean Squared Error (MSE)  | Average squared difference |\n",
        "| Binary Classification | Binary Cross-Entropy | Measures difference between predicted probabilities and true class |\n",
        "| Multi-class Classification | Categorical Cross-Entropy | Similar to binary but for multiple classes |\n",
        "\n",
        "**Mathematical equation**\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2\n",
        "$$\n",
        "\n",
        "* $y_i$: True value\n",
        "* $\\hat{y_i}$: Predicted value\n",
        "* $n$: Number of samples"
      ],
      "metadata": {
        "id": "O6LhqKXAdkx3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 8. What are some common types of loss functions?\n",
        "**Ans** - Some common types of loss functions used in neural networks, categorized by problem type.\n",
        "\n",
        "1. **Loss Functions for Regression**\n",
        "\n",
        "| Loss Function | Formula | Description | Use Case |\n",
        "|-||||\n",
        "| Mean Squared Error (MSE)  | $\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2$   | Penalizes larger errors more heavily  | Predicting continuous values  |\n",
        "| Mean Absolute Error (MAE) | ( \\frac{1}{n} \\sum\\_{i=1}^n | y\\_i - \\hat{y\\_i} | ) | Measures average absolute difference | Regression with outlier robustness |\n",
        "| Huber Loss | Combines MSE and MAE, less sensitive to outliers | Smooth transition between MAE and MSE | Regression with some outliers |\n",
        "\n",
        "2. **Loss Functions for Classification**\n",
        "\n",
        "| Loss Function | Formula | Description | Use Case |\n",
        "|-||||\n",
        "| **Binary Cross-Entropy (Log Loss)**  | $-\\frac{1}{n} \\sum_{i=1}^n [y_i \\log(\\hat{y_i}) + (1 - y_i) \\log(1 - \\hat{y_i})]$ | Measures error between predicted probability and true class | Binary classification (e.g., spam detection) |\n",
        "| Categorical Cross-Entropy | $-\\sum_{c=1}^C y_c \\log(\\hat{y_c})$ | Extension of binary cross-entropy for multi-class problems  | Multi-class classification (e.g., digit recognition) |\n",
        "| **Sparse Categorical Cross-Entropy** | Similar to categorical cross-entropy but works with integer labels | Efficient for multi-class classification with large classes | Same as above |\n",
        "\n",
        "3. **Loss Functions for Special Cases**\n",
        "\n",
        "| Loss Function | Description | Use Case |\n",
        "|-|||\n",
        "| Hinge Loss | Used for \"maximum-margin\" classification | Binary classification, especially with margin-based methods |\n",
        "| **KL Divergence** | Measures difference between two probability distributions | Tasks involving probabilistic outputs or distributions |"
      ],
      "metadata": {
        "id": "NVzGYK9VdlB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 9. How does a neural network learn?\n",
        "**Ans** -Neural network learns in following ways\n",
        "\n",
        "1. **Initialization:**\n",
        "\n",
        "   * The network starts with random weights and biases.\n",
        "\n",
        "2. **Forward Propagation:**\n",
        "\n",
        "   * Input data is passed through the network layer by layer.\n",
        "   * Each neuron computes a weighted sum of its inputs, adds a bias, and applies an activation function.\n",
        "   * This produces the output prediction.\n",
        "\n",
        "3. **Loss Calculation:**\n",
        "\n",
        "   * The network's prediction is compared to the true target using a loss function.\n",
        "   * The loss quantifies the error between prediction and truth.\n",
        "\n",
        "4. **Backward Propagation**\n",
        "\n",
        "   * The network calculates the gradient of the loss with respect to each weight and bias.\n",
        "   * This uses the chain rule of calculus to propagate the error backward through the network.\n",
        "   * Gradients indicate how much and in which direction to change each parameter to reduce the loss.\n",
        "\n",
        "5. **Parameter Update:**\n",
        "\n",
        "   * Using an optimization algorithm, the weights and biases are updated.\n",
        "   * Example update rule:\n",
        "\n",
        "   $$\n",
        "   w := w - \\eta \\frac{\\partial L}{\\partial w}\n",
        "   $$\n",
        "\n",
        "   Where:\n",
        "\n",
        "   * $w$ is a weight\n",
        "   * $\\eta$ is the learning rate (step size)\n",
        "   * $\\frac{\\partial L}{\\partial w}$ is the gradient of loss with respect to $w$\n",
        "\n",
        "6. **Iteration:**\n",
        "\n",
        "   * Steps 2 to 5 are repeated over many epochs.\n",
        "   * Over time, the network's predictions improve as the loss decreases.\n",
        "\n",
        "**Intuition**\n",
        "  * The network learns by trial and error, adjusting weights to reduce mistakes.\n",
        "  * Backpropagation provides the feedback signal needed to improve.\n",
        "  * The learning rate controls how big the steps are for updates."
      ],
      "metadata": {
        "id": "cm7GwkSadlUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 10. What is an optimizer in neural networks, and why is it necessary?\n",
        "**Ans** - An optimizer in neural networks is an algorithm or method used to adjust the network's weights and biases during training to minimize the loss function.\n",
        "\n",
        "**An Optimizer is Necessary**\n",
        "* After computing the loss and its gradients, we need a systematic way to update the model parameters to reduce the error.\n",
        "* The optimizer decides how much and in which direction to change each weight and bias.\n",
        "* Without an optimizer, the network wouldn't know how to improve or converge towards better performance.\n",
        "\n",
        "**Working of an Optimizer**\n",
        "\n",
        "* It takes the gradients of the loss with respect to each parameter.\n",
        "* It updates the parameters step-by-step to minimize the loss.\n",
        "* It can include additional techniques like:\n",
        "\n",
        "  * Adjusting the step size\n",
        "  * Momentum\n",
        "  * Adaptive learning rates per parameter\n",
        "\n",
        "**Common Optimizers**\n",
        "\n",
        "| Optimizer | Description |\n",
        "|-||\n",
        "|Gradient Descent | Updates parameters by moving against the gradient. Can be slow with large datasets.         |\n",
        "| Stochastic Gradient Descent | Uses one or a few training samples per update for faster, noisier updates.                  |\n",
        "| Momentum | Adds a fraction of the previous update to smooth progress and accelerate convergence.       |\n",
        "| Adam | Combines momentum and adaptive learning rates, widely used for fast and effective training. |\n",
        "| RMSprop | Adapts learning rate for each parameter based on recent gradients, good for recurrent nets. |"
      ],
      "metadata": {
        "id": "zHTfaQUOdlmO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 11. Could you briefly describe some common optimizers?\n",
        "**Ans** - Some common neural network optimizers\n",
        "\n",
        "1. **Gradient Descent**\n",
        "* **It's working:** Updates weights by computing the gradient of the loss over the entire training dataset.\n",
        "* **Pros:** Simple and straightforward.\n",
        "* **Cons:** Can be very slow for large datasets since it processes all data each step.\n",
        "\n",
        "2. **Stochastic Gradient Descent**\n",
        "* **It's working:** Updates weights using the gradient from one training example at a time.\n",
        "* **Pros:** Faster updates and can escape local minima due to noisy updates.\n",
        "* **Cons:** Noisy updates can cause fluctuation and slow convergence.\n",
        "\n",
        "3. **Momentum**\n",
        "* **It's working:** Builds on SGD by adding a “momentum” term that helps accelerate updates in consistent gradient directions.\n",
        "* **Pros:** Faster convergence and smoother updates.\n",
        "\n",
        "4. **RMSprop**\n",
        "* **It's working:** Adjusts the learning rate for each parameter individually, scaling it inversely proportional to the root mean square of recent gradients.\n",
        "* **Pros:** Works well for problems with non-stationary objectives like RNNs.\n",
        "\n",
        "5. **Adam (Adaptive Moment Estimation)**\n",
        "* **It's working:** Combines ideas from Momentum and RMSprop by keeping running averages of both gradients and their squares.\n",
        "* **Pros:** Fast convergence, adaptive learning rates, works well on a wide variety of problems.\n",
        "* **Most popular** optimizer in practice.\n",
        "\n",
        "**Table**\n",
        "\n",
        "| Optimizer | Key Feature                  | Best For                        |\n",
        "| --------- | ---------------------------- | ------------------------------- |\n",
        "| GD        | Full dataset updates         | Small datasets, simple problems |\n",
        "| SGD       | Single or mini-batch updates | Large datasets, noisy gradient  |\n",
        "| Momentum  | Adds velocity to updates     | Faster, smoother convergence    |\n",
        "| RMSprop   | Adaptive learning rates      | RNNs, non-stationary objectives |\n",
        "| Adam      | Combines momentum & RMSprop  | Most deep learning tasks        |"
      ],
      "metadata": {
        "id": "DjQcXn0_dl3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 12. Can you explain forward and backward propagation in a neural network?\n",
        "**Ans** - A clear explanation of forward propagation and backward propagation in a neural network:\n",
        "\n",
        "**Forward Propagation**\n",
        "* The process of passing input data through the neural network to get an output.\n",
        "\n",
        "* **It's working:**\n",
        "  1. Input data $x$ is fed into the input layer.\n",
        "  2. Each neuron calculates a weighted sum of its inputs plus a bias.\n",
        "  3. An **activation function** is applied to this sum to produce the neuron's output.\n",
        "  4. This output becomes the input to the next layer.\n",
        "  5. The process repeats layer by layer until the output layer produces the final prediction $\\hat{y}$.\n",
        "\n",
        "* **Purpose:** To generate predictions from the current state of the network.\n",
        "\n",
        "**Backward Propagation**\n",
        "\n",
        "* The process of updating the network's weights and biases based on the error in the prediction.\n",
        "\n",
        "* **It's working:**\n",
        "  1. Compute the loss by comparing the predicted output $\\hat{y}$ with the true output $y$ using a loss function.\n",
        "  2. Calculate the gradient of the loss with respect to each weight and bias in the network using the chain rule of calculus.\n",
        "  3. Propagate these gradients backwards through the network from the output layer to the input layer.\n",
        "  4. Use these gradients to update the weights and biases, typically using an optimizer like Gradient Descent.\n",
        "\n",
        "* **Purpose:** To minimize the loss by adjusting the network's parameters, improving accuracy over time."
      ],
      "metadata": {
        "id": "G6pektVedmHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 13. What is weight initialization, and how does it impact training?\n",
        "**Ans** - Great question! Here's a clear explanation:\n",
        "\n",
        "---\n",
        "\n",
        "## What is **Weight Initialization**?\n",
        "\n",
        "**Weight initialization** is the process of setting the starting values for the weights in a neural network before training begins.\n",
        "\n",
        "* Instead of starting all weights at zero or the same value, we initialize them (usually randomly) to break symmetry and enable effective learning.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Does Weight Initialization Matter?\n",
        "\n",
        "1. **Breaks Symmetry:**\n",
        "\n",
        "   * If all weights start the same (e.g., zeros), neurons in the same layer learn the same features, making learning ineffective.\n",
        "   * Random initialization ensures neurons learn different features.\n",
        "\n",
        "2. **Controls Signal Flow:**\n",
        "\n",
        "   * Proper initialization keeps the input signals flowing well through the network (not too big or too small).\n",
        "   * Prevents **vanishing** or **exploding gradients**, where gradients become too small or too large during training, causing slow or unstable learning.\n",
        "\n",
        "3. **Speeds Up Convergence:**\n",
        "\n",
        "   * Good initialization helps the model train faster and reach better accuracy by starting closer to an optimal solution.\n",
        "\n",
        "**Common Initialization Methods**\n",
        "\n",
        "| Method | Description | Use Case |\n",
        "|-|||\n",
        "| Random Initialization | Small random values (e.g., Gaussian with mean 0) | Simple but can cause problems in deep nets |\n",
        "| Xavier/Glorot Initialization | Scales weights based on the number of input and output neurons to maintain variance | For sigmoid/tanh activations |\n",
        "| He Initialization  | Similar to Xavier but scaled for ReLU activations | For ReLU and variants |"
      ],
      "metadata": {
        "id": "WYykcaPadmZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 14. What is the vanishing gradient problem in deep learning?\n",
        "**Ans** - The vanishing gradient problem is a common issue in training deep neural networks, especially those with many layers.\n",
        "* During backpropagation, the gradients are propagated backward through the network layers.\n",
        "* If these gradients become very small as they move toward the earlier layers, the weights in those layers get updated very little or not at all.\n",
        "* This slows down or completely stalls learning in the early layers of the network.\n",
        "\n",
        "**It Happens**\n",
        "* It often occurs when using certain activation functions like sigmoid or tanh.\n",
        "* These functions squash input values into a small range.\n",
        "* Their derivatives are also small.\n",
        "* When multiplied repeatedly during backpropagation through many layers, the gradients shrink exponentially.\n",
        "\n",
        "**Consequences**\n",
        "* Early layers learn very slowly or stop learning altogether.\n",
        "* The network fails to capture important low-level features.\n",
        "* Training becomes inefficient or ineffective for very deep networks.\n",
        "\n",
        "**Mitigation of Vanishing Gradients**\n",
        "* Use activation functions like ReLU which do not squash gradients as much.\n",
        "* Use proper weight initialization methods.\n",
        "* Employ architectures like ResNets with skip connections that allow gradients to flow more easily.\n",
        "* Use batch normalization to stabilize gradient flow."
      ],
      "metadata": {
        "id": "1M7yRyBQdmqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 15. What is the exploding gradient problem?\n",
        "**Ans** - The exploding gradient problem is another common challenge when training deep neural networks, kind of the opposite of the vanishing gradient problem.\n",
        "* During backpropagation, the gradients can sometimes become very large, growing exponentially as they are propagated backward through many layers.\n",
        "* When gradients explode, the weight updates become too big.\n",
        "* This causes the network's parameters to change wildly, leading to:\n",
        "  * **Unstable training**\n",
        "  * **Loss values that fluctuate or become NaN**\n",
        "  * Failure to converge to a good solution\n",
        "\n",
        "**It Happens**\n",
        "* Happens when activation functions or weight initialization cause the gradients to amplify during backpropagation.\n",
        "* Common in very deep networks or recurrent neural networks where many layers or time steps multiply gradients repeatedly.\n",
        "\n",
        "**Consequences**\n",
        "* Training becomes unstable or divergent.\n",
        "* Model fails to learn meaningful patterns.\n",
        "* Loss can suddenly jump or become infinite.\n",
        "\n",
        "**Mitigation of Exploding Gradients**\n",
        "\n",
        "* Gradient Clipping: Limit the gradients to a maximum threshold during training.\n",
        "* Proper Weight Initialization: Use methods like Xavier or He initialization to keep gradients stable.\n",
        "* Use architectures designed to handle deep gradients: Like LSTM or GRU in RNNs.\n",
        "* Batch Normalization: Helps stabilize the gradient flow."
      ],
      "metadata": {
        "id": "WHN5DeZ-dm72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "eQjsC3PqdnPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 1. How do we create a simple perceptron for basic binary classification?\n",
        "**Ans** - A perceptron is the simplest type of neural network—a single-layer network with one neuron.\n",
        "* It takes multiple inputs, applies weights, sums them, adds a bias, and passes the result through an activation function.\n",
        "* It outputs either 0 or 1, making it suitable for binary classification.\n",
        "\n",
        "**Steps to Create a Simple Perceptron**\n",
        "1. **Initialize weights and bias**\n",
        "\n",
        "2. For each training example:\n",
        "\n",
        "   * Calculate the weighted sum of inputs + bias.\n",
        "   * Apply the step activation function:\n",
        "\n",
        "     * Output = 1 if weighted sum ≥ 0\n",
        "     * Output = 0 if weighted sum < 0\n",
        "\n",
        "3. Compare the predicted output to the true label.\n",
        "\n",
        "4. Update weights and bias based on the error using the Perceptron learning rule\n",
        "\n",
        "   $$\n",
        "   w_i := w_i + \\eta \\times (y - \\hat{y}) \\times x_i\n",
        "   $$\n",
        "\n",
        "   $$\n",
        "   b := b + \\eta \\times (y - \\hat{y})\n",
        "   $$\n",
        "\n",
        "   Where:\n",
        "\n",
        "   * $w_i$ is weight for input $i$\n",
        "   * $\\eta$ is the learning rate\n",
        "   * $y$ is true label\n",
        "   * $\\hat{y}$ is predicted label\n",
        "   * $x_i$ is input feature $i$\n",
        "\n",
        "5. Repeat for multiple epochs until convergence."
      ],
      "metadata": {
        "id": "18n0GnAidnhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Perceptron:\n",
        "    def __init__(self, input_size, learning_rate=0.1, epochs=10):\n",
        "        self.weights = np.zeros(input_size)\n",
        "        self.bias = 0\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def activation(self, x):\n",
        "        return 1 if x >= 0 else 0\n",
        "\n",
        "    def predict(self, x):\n",
        "        linear_output = np.dot(x, self.weights) + self.bias\n",
        "        return self.activation(linear_output)\n",
        "\n",
        "    def train(self, X, y):\n",
        "        for _ in range(self.epochs):\n",
        "            for inputs, label in zip(X, y):\n",
        "                prediction = self.predict(inputs)\n",
        "                error = label - prediction\n",
        "                self.weights += self.lr * error * inputs\n",
        "                self.bias += self.lr * error\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 0, 0, 1])  # AND logic gate\n",
        "\n",
        "perceptron = Perceptron(input_size=2)\n",
        "perceptron.train(X, y)\n",
        "\n",
        "print([perceptron.predict(x) for x in X])  # Outputs: [0, 0, 0, 1]"
      ],
      "metadata": {
        "id": "tedtlpVtZtG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 2. How can you build a neural network with one hidden layer using Keras?\n",
        "**Ans** - **Steps to Build a Neural Network with One Hidden Layer in Keras**\n",
        "\n",
        "1. Import necessary modules\n",
        "2. Prepare our data\n",
        "3. Define the model architecture\n",
        "4. Compile the model\n",
        "5. Train the model\n",
        "6. Evaluate or predict"
      ],
      "metadata": {
        "id": "E5h_ICUgdn2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "# Example data (X: inputs, y: labels)\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([0, 1, 1, 0])  # XOR problem (not linearly separable)\n",
        "\n",
        "# 1. Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer + hidden layer with 4 neurons, using ReLU activation\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))\n",
        "\n",
        "# Output layer with 1 neuron (binary classification), using sigmoid activation\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 2. Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 3. Train the model\n",
        "model.fit(X, y, epochs=100, batch_size=1, verbose=1)\n",
        "\n",
        "# 4. Evaluate the model\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "# 5. Predict\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\", (predictions > 0.5).astype(int))"
      ],
      "metadata": {
        "id": "MVoRorAVI9iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation**\n",
        "* `Sequential()`: Creates a linear stack of layers.\n",
        "* `Dense(4, input_dim=2, activation='relu')`: A hidden layer with 4 neurons, input dimension 2, and ReLU activation.\n",
        "* `Dense(1, activation='sigmoid')`: Output layer with 1 neuron for binary classification.\n",
        "* `compile`: Defines loss function (binary crossentropy), optimizer (Adam), and metric (accuracy).\n",
        "* `fit`: Trains the model on the data.\n",
        "* `predict`: Predicts output for inputs."
      ],
      "metadata": {
        "id": "SPqvDDdNJCYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 3. How do you initialize weights using the Xavier (Glorot) initialization method in Keras?\n",
        "**Ans** - It initializes weights by keeping the variance of activations the same across every layer.\n",
        "* This helps avoid vanishing or exploding gradients, especially for sigmoid or tanh activations.\n",
        "\n",
        "**Use of Xavier Initialization in Keras**\n",
        "\n",
        "Keras provides built-in initializers:\n",
        "* `glorot_uniform` — Xavier initialization with uniform distribution\n",
        "* `glorot_normal` — Xavier initialization with normal distribution\n",
        "\n",
        "we specify these in our layer like this:"
      ],
      "metadata": {
        "id": "28Dm0VWRdoH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.initializers import glorot_uniform, glorot_normal\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Using glorot_uniform initializer for weights\n",
        "model.add(Dense(64, input_dim=100, activation='relu',\n",
        "                kernel_initializer=glorot_uniform()))\n",
        "\n",
        "# Or using glorot_normal initializer\n",
        "model.add(Dense(64, activation='relu',\n",
        "                kernel_initializer=glorot_normal()))"
      ],
      "metadata": {
        "id": "cFezrs3AISRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 4. How can you apply different activation functions in a neural network in Keras?\n",
        "**Ans** - Applying different activation functions in a neural network using Keras is straightforward! we specify the activation function for each layer when we define it.\n",
        "\n",
        "**Apply Activation Functions in Keras**\n",
        "\n",
        "When we add a layer like `Dense`, we use the `activation` parameter to set the activation function.\n",
        "\n",
        "**Example:**"
      ],
      "metadata": {
        "id": "Gaup5grudobu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input layer + hidden layer with ReLU activation\n",
        "model.add(Dense(32, input_dim=10, activation='relu'))\n",
        "\n",
        "# Another hidden layer with tanh activation\n",
        "model.add(Dense(16, activation='tanh'))\n",
        "\n",
        "# Output layer with sigmoid activation (commonly used for binary classification)\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "VHXJ7fBOFzOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Common Activation Functions in Keras:**\n",
        "\n",
        "| Activation Function | Keras String | Use Case |\n",
        "|-|||\n",
        "| ReLU | `'relu'` | Most popular for hidden layers |\n",
        "| Sigmoid | `'sigmoid'` | Binary classification output |\n",
        "| Tanh | `'tanh'` | Hidden layers, outputs in \\[-1,1] |\n",
        "| Softmax | `'softmax'` | Multi-class classification output |\n",
        "| Linear (no activation) | `None` or `'linear'` | Regression or no activation |\n",
        "| LeakyReLU | Use `LeakyReLU` layer | When we want a variant of ReLU |\n",
        "\n",
        "**Using Advanced Activations (like LeakyReLU):**"
      ],
      "metadata": {
        "id": "hyFwT3zJF4HF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import LeakyReLU\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim=10))\n",
        "model.add(LeakyReLU(alpha=0.1))  # Leaky ReLU after Dense layer"
      ],
      "metadata": {
        "id": "f0XnISpsGnWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 5. How do you add dropout to a neural network model to prevent overfitting?\n",
        "**Ans** - Adding dropout to our neural network is a great way to reduce overfitting by randomly \"dropping out\" a fraction of neurons during training, which forces the network to learn more robust features.\n",
        "\n",
        "**Add Dropout in Keras**\n",
        "\n",
        "we simply insert a Dropout layer between our Dense layers.\n",
        "\n",
        "**Example:**"
      ],
      "metadata": {
        "id": "j1PJBE2Ndosv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Input + first hidden layer\n",
        "model.add(Dense(64, activation='relu', input_dim=100))\n",
        "\n",
        "# Add dropout layer with 30% dropout rate\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "# Second hidden layer\n",
        "model.add(Dense(32, activation='relu'))\n",
        "\n",
        "# Another dropout layer with 20% dropout rate\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Output layer for binary classification\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "_xbNzBd-FXlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Points about Dropout:**\n",
        "* The argument to `Dropout()` is the dropout rate, e.g., `0.3` means randomly dropping 30% of the neurons in that layer during each training step.\n",
        "* Dropout is only active during training, not during evaluation or prediction.\n",
        "* Usually added after activation layers.\n",
        "* Helps prevent overfitting by reducing co-adaptation of neurons."
      ],
      "metadata": {
        "id": "jvKtnKlYFbqt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 6. How do you manually implement forward propagation in a simple neural network?\n",
        "**Ans** - We can manually implement forward propagation in a simple neural network from scratch using Python and NumPy.\n",
        "* It's the process where input data passes through the network layer-by-layer.\n",
        "* Each layer applies weights, biases, and activation functions to produce output.\n",
        "* The final output layer produces predictions.\n",
        "\n",
        "**Example: Neural Network with**\n",
        "* Input layer\n",
        "* One hidden layer\n",
        "* Output layer\n",
        "\n",
        "**Step-by-step Python Code**"
      ],
      "metadata": {
        "id": "q9N7jdcWdo-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Input features (example)\n",
        "X = np.array([0.5, 0.1])  # shape (2,)\n",
        "\n",
        "# Weights and biases initialization (random for example)\n",
        "W1 = np.array([[0.2, -0.4],\n",
        "               [0.7, 0.1],\n",
        "               [-0.5, 0.3]])   # shape (3,2) for 3 neurons, 2 inputs\n",
        "\n",
        "b1 = np.array([0.1, 0.2, -0.1])  # shape (3,)\n",
        "\n",
        "W2 = np.array([[0.6, -0.1, 0.2]])  # shape (1,3) for 1 output neuron\n",
        "b2 = np.array([0.05])               # shape (1,)\n",
        "\n",
        "# Activation functions\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Forward propagation\n",
        "\n",
        "# Layer 1 (hidden layer)\n",
        "Z1 = np.dot(W1, X) + b1          # Linear step: (3,) = (3,2)·(2,) + (3,)\n",
        "A1 = relu(Z1)                    # Activation: ReLU\n",
        "\n",
        "# Layer 2 (output layer)\n",
        "Z2 = np.dot(W2, A1) + b2         # Linear step: (1,) = (1,3)·(3,) + (1,)\n",
        "A2 = sigmoid(Z2)                 # Activation: Sigmoid (output probability)\n",
        "\n",
        "print(\"Hidden layer activations:\", A1)\n",
        "print(\"Output:\", A2)"
      ],
      "metadata": {
        "id": "F39ZcDmLE9hM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "* `Z1` = weighted sum + bias for hidden layer.\n",
        "* `A1` = activation output of hidden layer.\n",
        "* `Z2` = weighted sum + bias for output layer.\n",
        "* `A2` = final output after sigmoid."
      ],
      "metadata": {
        "id": "NgchrMRzFE5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 7. How do you add batch normalization to a neural network model in Keras?\n",
        "**Ans** - Adding Batch Normalization in Keras is simple and powerful! It helps stabilize and speed up training by normalizing layer inputs.\n",
        "\n",
        "**Adding Batch Normalization in Keras**\n",
        "\n",
        "we just insert a `BatchNormalization` layer between our layers, typically after the Dense layer and before the activation function.\n",
        "\n",
        "**Example Code:**"
      ],
      "metadata": {
        "id": "ENVFREkJdpQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Activation\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Dense layer\n",
        "model.add(Dense(64, input_dim=100))\n",
        "\n",
        "# Batch Normalization\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Activation function (ReLU)\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "u4xrSGAeEM46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Points**\n",
        "\n",
        "* BatchNorm normalizes activations to zero mean and unit variance per mini-batch.\n",
        "* Typically added before or after activation.\n",
        "* Helps reduce internal covariate shift.\n",
        "* Often improves training speed and performance.\n",
        "* Can be used with any type of layer."
      ],
      "metadata": {
        "id": "yKSv_VknEWVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 8. How can you visualize the training process with accuracy and loss curves?\n",
        "**Ans** - Visualizing training progress with accuracy and loss curves is super helpful to understand how our neural network is learning and whether it's overfitting or underfitting.\n",
        "\n",
        "**Visualizing accuracy and loss curves in Keras**\n",
        "\n",
        "When we train a model with `model.fit()`, Keras returns a History object containing training metrics for every epoch.\n",
        "\n",
        "we can plot these using Matplotlib.\n",
        "\n",
        "**Example:**"
      ],
      "metadata": {
        "id": "-JB_JgNddpft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming we already have our model and data\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=32)\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(12,5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5qZpUUIhDuH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Accuracy plot: Shows how well our model is performing on training and validation data over epochs.\n",
        "* Loss plot: Shows how the error is decreasing during training and validation."
      ],
      "metadata": {
        "id": "7zWXrElUDyjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 9. How can we use gradient clipping in Keras to control the gradient size and prevent exploding gradients?\n",
        "**Ans** - Gradient clipping is a technique used to prevent exploding gradients by capping the gradients during backpropagation to a maximum value or norm.\n",
        "\n",
        "**Use Gradient Clipping in Keras**\n",
        "\n",
        "we apply gradient clipping through the optimizer by setting either:\n",
        "\n",
        "* `clipnorm`: Clips gradients by their norm\n",
        "* `clipvalue`: Clips gradients by their absolute value\n",
        "\n",
        "**Example: Clipping by Norm**"
      ],
      "metadata": {
        "id": "vNx1qJs3dpwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Create Adam optimizer with gradient clipping by norm (e.g., max norm = 1.0)\n",
        "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "YTyukmjMCqgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example: Clipping by Value**"
      ],
      "metadata": {
        "id": "oFqWma4NCv5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(learning_rate=0.001, clipvalue=0.5)\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "UgjA4PvEC7ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 10. How can you create a custom loss function in Keras?\n",
        "**Ans** - Creating a custom loss function in Keras is pretty straightforward we define a function that takes the true labels and predicted outputs as inputs and returns a scalar tensor representing the loss.\n",
        "\n",
        "**Create a custom loss function in Keras**\n",
        "1. Define a function with signature:"
      ],
      "metadata": {
        "id": "Tlk2fxkOdqB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    # compute loss\n",
        "    return loss_value"
      ],
      "metadata": {
        "id": "Mru4XhzcAzZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `y_true`: ground truth labels\n",
        "* `y_pred`: model predictions\n",
        "* Return a tensor representing the loss value\n",
        "\n",
        "2. Use TensorFlow backend inside for differentiability\n",
        "\n",
        "Example: A simple mean squared error custom loss"
      ],
      "metadata": {
        "id": "Rdpp8taKA49V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def custom_mse_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_true - y_pred))"
      ],
      "metadata": {
        "id": "4nkb6jLHBBKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Pass it to `model.compile()`"
      ],
      "metadata": {
        "id": "4VK48ZoFBG2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=custom_mse_loss, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "_8s7m1zvBKbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More complex example: Custom loss with penalty term**"
      ],
      "metadata": {
        "id": "Df1989KCBPKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss_with_penalty(y_true, y_pred):\n",
        "    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n",
        "    penalty = 0.1 * tf.reduce_mean(tf.abs(y_pred))  # example penalty\n",
        "    return mse + penalty"
      ],
      "metadata": {
        "id": "g6dncvEYBaEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 11. How can you visualize the structure of a neural network model in Keras?\n",
        "**Ans** - To visualize the structure of a neural network model in Keras, we can use the built-in utility `plot_model` from `tensorflow.keras.utils`. It creates a neat diagram showing layers, shapes, and connections.\n",
        "\n",
        "**Visualizing a Keras model architecture**\n",
        "\n",
        "Step 1: Import `plot_model`"
      ],
      "metadata": {
        "id": "gCE59tnIdqT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "EuEjtmJm_7zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: After building our model, call `plot_model`:"
      ],
      "metadata": {
        "id": "ECQhY_-J___e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "17tRd9cyAFs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `to_file` — filename to save the image (e.g., PNG, SVG)\n",
        "* `show_shapes=True` — display output shapes of each layer\n",
        "* `show_layer_names=True` — display layer names\n",
        "\n",
        "Step 3: View the generated image file (`model_architecture.png`)\n",
        "\n",
        "Example usage:"
      ],
      "metadata": {
        "id": "d_HZBVb7APCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(100,)),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "plot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)"
      ],
      "metadata": {
        "id": "PsYQgWzqAXTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}